{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter =0, Testing Accuracy =0.952,Learning Rate = 0.001\n",
      "Iter =1, Testing Accuracy =0.9617,Learning Rate = 0.00095\n",
      "Iter =2, Testing Accuracy =0.9678,Learning Rate = 0.0009025\n",
      "Iter =3, Testing Accuracy =0.9688,Learning Rate = 0.000857375\n",
      "Iter =4, Testing Accuracy =0.9705,Learning Rate = 0.00081450626\n",
      "Iter =5, Testing Accuracy =0.9726,Learning Rate = 0.0007737809\n",
      "Iter =6, Testing Accuracy =0.9734,Learning Rate = 0.0007350919\n",
      "Iter =7, Testing Accuracy =0.9771,Learning Rate = 0.0006983373\n",
      "Iter =8, Testing Accuracy =0.975,Learning Rate = 0.0006634204\n",
      "Iter =9, Testing Accuracy =0.9784,Learning Rate = 0.0006302494\n",
      "Iter =10, Testing Accuracy =0.9783,Learning Rate = 0.0005987369\n",
      "Iter =11, Testing Accuracy =0.9783,Learning Rate = 0.0005688001\n",
      "Iter =12, Testing Accuracy =0.98,Learning Rate = 0.0005403601\n",
      "Iter =13, Testing Accuracy =0.9778,Learning Rate = 0.0005133421\n",
      "Iter =14, Testing Accuracy =0.9805,Learning Rate = 0.000487675\n",
      "Iter =15, Testing Accuracy =0.9796,Learning Rate = 0.00046329122\n",
      "Iter =16, Testing Accuracy =0.9797,Learning Rate = 0.00044012666\n",
      "Iter =17, Testing Accuracy =0.9808,Learning Rate = 0.00041812033\n",
      "Iter =18, Testing Accuracy =0.9824,Learning Rate = 0.00039721432\n",
      "Iter =19, Testing Accuracy =0.9817,Learning Rate = 0.0003773536\n",
      "Iter =20, Testing Accuracy =0.981,Learning Rate = 0.00035848594\n",
      "Iter =21, Testing Accuracy =0.9795,Learning Rate = 0.00034056162\n",
      "Iter =22, Testing Accuracy =0.9809,Learning Rate = 0.00032353355\n",
      "Iter =23, Testing Accuracy =0.9822,Learning Rate = 0.00030735688\n",
      "Iter =24, Testing Accuracy =0.9829,Learning Rate = 0.000291989\n",
      "Iter =25, Testing Accuracy =0.9797,Learning Rate = 0.00027738957\n",
      "Iter =26, Testing Accuracy =0.9819,Learning Rate = 0.0002635201\n",
      "Iter =27, Testing Accuracy =0.983,Learning Rate = 0.00025034408\n",
      "Iter =28, Testing Accuracy =0.9804,Learning Rate = 0.00023782688\n",
      "Iter =29, Testing Accuracy =0.9823,Learning Rate = 0.00022593554\n",
      "Iter =30, Testing Accuracy =0.9805,Learning Rate = 0.00021463877\n",
      "Iter =31, Testing Accuracy =0.9811,Learning Rate = 0.00020390682\n",
      "Iter =32, Testing Accuracy =0.9826,Learning Rate = 0.00019371149\n",
      "Iter =33, Testing Accuracy =0.9824,Learning Rate = 0.0001840259\n",
      "Iter =34, Testing Accuracy =0.9827,Learning Rate = 0.00017482461\n",
      "Iter =35, Testing Accuracy =0.9825,Learning Rate = 0.00016608338\n",
      "Iter =36, Testing Accuracy =0.9823,Learning Rate = 0.00015777921\n",
      "Iter =37, Testing Accuracy =0.9807,Learning Rate = 0.00014989026\n",
      "Iter =38, Testing Accuracy =0.9824,Learning Rate = 0.00014239574\n",
      "Iter =39, Testing Accuracy =0.983,Learning Rate = 0.00013527596\n",
      "Iter =40, Testing Accuracy =0.9825,Learning Rate = 0.00012851215\n",
      "Iter =41, Testing Accuracy =0.9817,Learning Rate = 0.00012208655\n",
      "Iter =42, Testing Accuracy =0.9822,Learning Rate = 0.00011598222\n",
      "Iter =43, Testing Accuracy =0.9825,Learning Rate = 0.00011018311\n",
      "Iter =44, Testing Accuracy =0.9827,Learning Rate = 0.000104673956\n",
      "Iter =45, Testing Accuracy =0.9828,Learning Rate = 9.944026e-05\n",
      "Iter =46, Testing Accuracy =0.9822,Learning Rate = 9.446825e-05\n",
      "Iter =47, Testing Accuracy =0.9828,Learning Rate = 8.974483e-05\n",
      "Iter =48, Testing Accuracy =0.983,Learning Rate = 8.525759e-05\n",
      "Iter =49, Testing Accuracy =0.9828,Learning Rate = 8.099471e-05\n",
      "Iter =50, Testing Accuracy =0.9822,Learning Rate = 7.6944976e-05\n"
     ]
    }
   ],
   "source": [
    "#載入數據集，one_hot是把数据转化为只有0和1的形式\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) #這步有時候會失效\n",
    "\n",
    "#因為數據集很大，故我們要用stochastic gradient descent，\n",
    "#會將資料集分批次（一次100张）放入神经网络进行训练，\n",
    "#並不會一次將所有資料拿來train (計算量很大)\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次，训练集数量mnist.train.num_examples \n",
    "# // 在python中表示取商\n",
    "n_batch = mnist.train.num_examples // batch_size      \n",
    "\n",
    "#定义兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28 * 28 = 784，None值变为100\n",
    "y = tf.placeholder(tf.float32, [None, 10]) #輸出層，有十個神經元，每個神經元有一個激活值，十個激活值排成一個 1*10的向量\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# lr即为learning rate，学习率\n",
    "lr = tf.Variable(0.001, dtype = tf.float32)  #初始学习率定义的是0.001\n",
    "\n",
    "\n",
    "#创建一個簡單的神經網路 (输入层784个神经元，输出层總共10個神經元，即十个标签)\n",
    "#隐藏层1\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 500],stddev = 0.1))              #权值\n",
    "    # b = tf.Variable(tf.zeros([1, 10]))                #偏置值\n",
    "b1 = tf.Variable(tf.zeros([500])+0.1)  \n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "#隐藏层2\n",
    "W2 = tf.Variable(tf.truncated_normal([500, 300],stddev = 0.1))              #权值\n",
    "b2 = tf.Variable(tf.zeros([300])+0.1)  \n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "#隐藏层3\n",
    "W3 = tf.Variable(tf.truncated_normal([300, 10],stddev = 0.1))              #权值\n",
    "b3 = tf.Variable(tf.zeros([10])+0.1)  \n",
    "prediction = tf.nn.softmax(tf.matmul(L2_drop, W3) + b3)  #预测值，用到softmax\n",
    "\n",
    "\n",
    "#交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction)) #交叉熵\n",
    "\n",
    "#训练\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "\n",
    "#初始化变量 \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#测试训练的准确率，求准确率的方法\n",
    "#如果y標籤最大的值，與prediction標籤最大的值相等，則回傳true\n",
    "#結果存在一個 boolean 的变量correct_prediction中\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "                              \n",
    " #argmax 返回一維張量中最大的值所在的位置\n",
    " # 求标签y里面最大的值在哪个位置即标签\n",
    " # tf.argmax(prediction, 1)预测 概率最大就会判定识别的这张图片是属于哪个标签的\n",
    " # (tf.argmax(y, 1)，真实样本的y存放的都是0或1，哪位是1就会返回哪位的值\n",
    " #  然后再比较上面两者，是否一样\n",
    "                             \n",
    "                              \n",
    "# 求准确率\n",
    "# 轉換資料格式 boolean 轉成 32位的float，接著再取平均值，得到准确率\n",
    "# true转换为1.0，false转换为0\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))         \n",
    "                              \n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(51): #总共疊代51次（周期） (outer loop)，把所有的图片训练51次\n",
    "        #每迭代一个周期，都会给学习率重新赋值\n",
    "        #0.95 ** epoch，指第epoch次迭代就是(0.95^epoch)，然后赋值给lr\n",
    "        # 学习率逐渐减小（最开始模型是比较混乱的，给它一个大的学习率可以让它快速收敛\n",
    "        #                 当收敛到全局最小值向其靠近的时候，就逐渐降低学习率，便于靠近）\n",
    "        sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))\n",
    "        \n",
    "        #每一次 outer loop 不一次拿所有的數據集，來做 Gradient desent，這就是 stochastic gradient descent\n",
    "        for batch in range(n_batch):  #每一個 outer loop 疊代 n_batch 個批次\n",
    "            #利用 train.next_batch 函數，讀取一個batch的 x, y 存給 batch_xs图片数据, batch_ys图片标签\n",
    "            # mnist.train.next_batch(batch_size)是获取下一个一百张图片               \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  \n",
    "                              \n",
    "              #拿來feed 的 dictionary                  \n",
    "            sess.run(train_step, feed_dict = {x: batch_xs, y: batch_ys, keep_prob: 1.0})\n",
    "        \n",
    "#         learnin_rate = sess.run(lr)   #手敲代码拼写单词的时候不小心少输一个字母结果程序报错\n",
    "        learning_rate = sess.run(lr)\n",
    "        #testing data feed dictionary\n",
    "        acc = sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n",
    "                   \n",
    "        # str(epoch)周期数\n",
    "        print(\"Iter =\" + str(epoch) + \", Testing Accuracy =\" + str(acc) + \",Learning Rate = \" + str(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
