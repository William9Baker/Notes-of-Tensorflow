{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-fb7d1a55610e>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\SOFTWARE\\Anaconda3_5.3.0\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\SOFTWARE\\Anaconda3_5.3.0\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\SOFTWARE\\Anaconda3_5.3.0\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\SOFTWARE\\Anaconda3_5.3.0\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\SOFTWARE\\Anaconda3_5.3.0\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-2-fb7d1a55610e>:103: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Iter =0, Testing Accuracy =0.8248\n",
      "Iter =1, Testing Accuracy =0.8938\n",
      "Iter =2, Testing Accuracy =0.9018\n",
      "Iter =3, Testing Accuracy =0.905\n",
      "Iter =4, Testing Accuracy =0.9089\n",
      "Iter =5, Testing Accuracy =0.911\n",
      "Iter =6, Testing Accuracy =0.9119\n",
      "Iter =7, Testing Accuracy =0.9134\n",
      "Iter =8, Testing Accuracy =0.9149\n",
      "Iter =9, Testing Accuracy =0.9162\n",
      "Iter =10, Testing Accuracy =0.9182\n",
      "Iter =11, Testing Accuracy =0.9178\n",
      "Iter =12, Testing Accuracy =0.9194\n",
      "Iter =13, Testing Accuracy =0.9191\n",
      "Iter =14, Testing Accuracy =0.9196\n",
      "Iter =15, Testing Accuracy =0.9202\n",
      "Iter =16, Testing Accuracy =0.9199\n",
      "Iter =17, Testing Accuracy =0.9213\n",
      "Iter =18, Testing Accuracy =0.9216\n",
      "Iter =19, Testing Accuracy =0.921\n",
      "Iter =20, Testing Accuracy =0.9216\n",
      "Iter =21, Testing Accuracy =0.9219\n",
      "Iter =22, Testing Accuracy =0.9227\n",
      "Iter =23, Testing Accuracy =0.9224\n",
      "Iter =24, Testing Accuracy =0.9234\n",
      "Iter =25, Testing Accuracy =0.9225\n",
      "Iter =26, Testing Accuracy =0.9232\n",
      "Iter =27, Testing Accuracy =0.924\n",
      "Iter =28, Testing Accuracy =0.9237\n",
      "Iter =29, Testing Accuracy =0.9234\n",
      "Iter =30, Testing Accuracy =0.923\n",
      "Iter =31, Testing Accuracy =0.924\n",
      "Iter =32, Testing Accuracy =0.9241\n",
      "Iter =33, Testing Accuracy =0.9254\n",
      "Iter =34, Testing Accuracy =0.9253\n",
      "Iter =35, Testing Accuracy =0.9254\n",
      "Iter =36, Testing Accuracy =0.9248\n",
      "Iter =37, Testing Accuracy =0.9259\n",
      "Iter =38, Testing Accuracy =0.9252\n",
      "Iter =39, Testing Accuracy =0.9262\n",
      "Iter =40, Testing Accuracy =0.9258\n",
      "Iter =41, Testing Accuracy =0.9261\n",
      "Iter =42, Testing Accuracy =0.9255\n",
      "Iter =43, Testing Accuracy =0.9261\n",
      "Iter =44, Testing Accuracy =0.9262\n",
      "Iter =45, Testing Accuracy =0.9257\n",
      "Iter =46, Testing Accuracy =0.9258\n",
      "Iter =47, Testing Accuracy =0.9267\n",
      "Iter =48, Testing Accuracy =0.9265\n",
      "Iter =49, Testing Accuracy =0.9259\n",
      "Iter =50, Testing Accuracy =0.927\n"
     ]
    }
   ],
   "source": [
    "#載入數據集，one_hot是把数据转化为只有0和1的形式\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) \n",
    "\n",
    "#會將資料集分批次（一次100张）放入神经网络进行训练，\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次，训练集数量mnist.train.num_examples \n",
    "# // 在python中表示取商\n",
    "n_batch = mnist.train.num_examples // batch_size      \n",
    "\n",
    "\n",
    "'''\n",
    "# 参数概要(视频上原版)\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)  #平均值，该方法记录这个值并给其名字\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stdev', stddev)       #标准差\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))  #最大值\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))  #最小值\n",
    "        tf.summary.scalar('histogram', var)     #直方图\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "#定义几个全局变量以及一些参数(修改版)\n",
    "global mean_scaler\n",
    "global stddev_scaler\n",
    "global max_scaler\n",
    "global min_scaler\n",
    "global histogram_scaler\n",
    "def variableSummaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            mean_scaler = tf.summary.scalar('mean',mean)#平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        stddev_scaler = tf.summary.scalar('stddev',stddev)#标准差\n",
    "        max_scaler = tf.summary.scalar('max',tf.reduce_max(var))#最大值\n",
    "        min_scaler = tf.summary.scalar('min',tf.reduce_min(var))#最小值\n",
    "        histogram_scaler = tf.summary.histogram('histogram',var)#直方图\n",
    "'''\n",
    "#命名空间\n",
    "with tf.name_scope('input'):\n",
    "    #定义兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x-input') # 28 * 28 = 784，None值变为100\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y-input') #輸出層，有十個神經元，每個神經元有一個激活值，十個激活值排成一個 1*10的向量\n",
    "\n",
    "with tf.name_scope('layer'):   \n",
    "    #创建一個簡單的神經網路 (只有输入层和輸出層，输入层784个神经元，输出层總共10個神經元，即十个标签)\n",
    "    with tf.name_scope('wights'):#命名空间下面的名门空间\n",
    "        W = tf.Variable(tf.zeros([784, 10]), name='W')  #权值\n",
    "        # 分析网络运行过程中权值的变化\n",
    "        #variable_summaries(W)\n",
    "        '''\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(W)\n",
    "            tf.summary.scalar('mean', mean)  #平均值，该方法记录这个值并给其名字\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(W - mean)))\n",
    "        tf.summary.scalar('stdev', stddev)       #标准差\n",
    "        tf.summary.scalar('max', tf.reduce_max(W))  #最大值\n",
    "        tf.summary.scalar('min', tf.reduce_min(W))  #最小值\n",
    "        tf.summary.scalar('histogram', W)     #直方图\n",
    "        '''\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(W)\n",
    "            meanW_scaler = tf.summary.scalar('mean',mean)#平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(W-mean)))\n",
    "        stddevW_scaler = tf.summary.scalar('stddev',stddev)#标准差\n",
    "        maxW_scaler = tf.summary.scalar('max',tf.reduce_max(W))#最大值\n",
    "        minW_scaler = tf.summary.scalar('min',tf.reduce_min(W))#最小值\n",
    "        histogramW_scaler = tf.summary.histogram('histogram',W)#直方图\n",
    "        \n",
    "        \n",
    "    with tf.name_scope('biases'):\n",
    "        b = tf.Variable(tf.zeros([10]), name='b') \n",
    "        #variable_summaries(b)\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(b)\n",
    "            mean_scaler = tf.summary.scalar('mean',mean)#平均值\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(b-mean)))\n",
    "        stddev_scaler = tf.summary.scalar('stddev',stddev)#标准差\n",
    "        max_scaler = tf.summary.scalar('max',tf.reduce_max(b))#最大值\n",
    "        min_scaler = tf.summary.scalar('min',tf.reduce_min(b))#最小值\n",
    "        histogram_scaler = tf.summary.histogram('histogram',b)#直方图\n",
    "        \n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(x, W) + b\n",
    "        #分析'wx_plus_b和'预测值prediction'没多大意义，这里只分析权值W和偏置值b\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(wx_plus_b)   #预测值，用到softmax\n",
    "\n",
    "\n",
    "#二次代價函數 : loss = mean((y - prediction)^2)\n",
    "#loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction)) #交叉熵\n",
    "    # tf.summary.scalar('loss',loss)  \n",
    "    loss_scaler = tf.summary.scalar('loss',loss)\n",
    "    \n",
    "    # 分析的数值比较多的时候才有必要调用variable_summaries（）函数\n",
    "with tf.name_scope('train'):\n",
    "    #使用梯度下降法\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "#初始化变量 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        # 結果存在一個 boolean 的变量correct_prediction中\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))                                                \n",
    "    with tf.name_scope('accuracy'):\n",
    "        # 求准确率\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))         \n",
    "        accuracy_scaler = tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "#合并所有的summary\n",
    "#merged = tf.summary.merge_all()     #报错的原因所在！！！\n",
    "\n",
    "\n",
    "#下面这些与网络结构没有关系，只有上面这些才会     \n",
    "# 開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph) #文件夹存放的是sess.graph图的结构\n",
    "    for epoch in range(51): #总共疊代51次，因为主要是使用tensorboard来画神经网络结构的\n",
    "        for batch in range(n_batch):#每一個 outer loop 疊代 n_batch 個批次\n",
    "            #利用 train.next_batch 函數，讀取一個batch的 x, y 存給 batch_xs图片数据, batch_ys图片标签\n",
    "            # mnist.train.next_batch(batch_size)是获取下一个一百张图片               \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)               \n",
    "           # summary,_ = sess.run([merged,train_step], feed_dict = {x: batch_xs, y: batch_ys}) #报错的原因所在之二！！！\n",
    "            # 一边训练，一边统计权值和偏置值的loss值和accuracy值\n",
    "            \n",
    "            # writer.add_summary(summary, epoch)    #报错的原因所在之三！！！\n",
    "            meanW_metall,stddevW_metall,maxW_metall,minW_metall,histogramW_metall,mean_metall,stddev_metall,max_metall,min_metall,histogram_metall,loss_metall,accuracy_metall,_ = sess.run([meanW_scaler,stddevW_scaler,maxW_scaler,minW_scaler,histogramW_scaler,mean_scaler,stddev_scaler,max_scaler,min_scaler,histogram_scaler,loss_scaler,accuracy_scaler,train_step], feed_dict={x:batch_xs,y:batch_ys})\n",
    "        writer.add_summary(meanW_metall,epoch)\n",
    "        writer.add_summary(stddevW_metall,epoch)\n",
    "        writer.add_summary(maxW_metall,epoch)\n",
    "        writer.add_summary(minW_metall,epoch)\n",
    "        writer.add_summary(histogramW_metall,epoch)\n",
    "        writer.add_summary(mean_metall,epoch)\n",
    "        writer.add_summary(stddev_metall,epoch)\n",
    "        writer.add_summary(max_metall,epoch)\n",
    "        writer.add_summary(min_metall,epoch)\n",
    "        writer.add_summary(histogram_metall,epoch)\n",
    "        writer.add_summary(loss_metall,epoch)\n",
    "        writer.add_summary(accuracy_metall,epoch)\n",
    "         \n",
    "    \n",
    "            \n",
    "        #每做完一次 outer loop 計算一次准确率\n",
    "        acc = sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels})\n",
    "                   \n",
    "        # str(epoch)周期数\n",
    "        print(\"Iter =\" + str(epoch) + \", Testing Accuracy =\" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
