{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter =0, Testing Accuracy =0.9196\n",
      "Iter =1, Testing Accuracy =0.9246\n",
      "Iter =2, Testing Accuracy =0.9238\n",
      "Iter =3, Testing Accuracy =0.9241\n",
      "Iter =4, Testing Accuracy =0.9265\n",
      "Iter =5, Testing Accuracy =0.9279\n",
      "Iter =6, Testing Accuracy =0.93\n",
      "Iter =7, Testing Accuracy =0.9304\n",
      "Iter =8, Testing Accuracy =0.9295\n",
      "Iter =9, Testing Accuracy =0.9289\n",
      "Iter =10, Testing Accuracy =0.931\n",
      "Iter =11, Testing Accuracy =0.9304\n",
      "Iter =12, Testing Accuracy =0.9314\n",
      "Iter =13, Testing Accuracy =0.933\n",
      "Iter =14, Testing Accuracy =0.9306\n",
      "Iter =15, Testing Accuracy =0.9331\n",
      "Iter =16, Testing Accuracy =0.9316\n",
      "Iter =17, Testing Accuracy =0.9321\n",
      "Iter =18, Testing Accuracy =0.9309\n",
      "Iter =19, Testing Accuracy =0.9288\n",
      "Iter =20, Testing Accuracy =0.9322\n"
     ]
    }
   ],
   "source": [
    "#載入數據集，one_hot是把数据转化为只有0和1的形式\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True) #這步有時候會失效\n",
    "\n",
    "#因為數據集很大，故我們要用stochastic gradient descent，\n",
    "#會將資料集分批次（一次100张）放入神经网络进行训练，\n",
    "#並不會一次將所有資料拿來train (計算量很大)\n",
    "#每一個批次的大小\n",
    "batch_size = 100 \n",
    "\n",
    "#計算一共有多少批次，训练集数量mnist.train.num_examples \n",
    "# // 在python中表示取商\n",
    "n_batch = mnist.train.num_examples // batch_size      \n",
    "\n",
    "#定义兩個placeholder，目的在於 train時候透過 feed 傳入 x_data 與 y_data\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 28 * 28 = 784，None值变为100\n",
    "y = tf.placeholder(tf.float32, [None, 10]) #輸出層，有十個神經元，每個神經元有一個激活值，十個激活值排成一個 1*10的向量\n",
    "\n",
    "#创建一個簡單的神經網路 (只有输入层和輸出層，输入层784个神经元，输出层總共10個神經元，即十个标签)\n",
    "W = tf.Variable(tf.zeros([784, 10]))              #权值\n",
    "    # b = tf.Variable(tf.zeros([1, 10]))                #偏置值\n",
    "b = tf.Variable(tf.zeros([10]))  \n",
    "prediction = tf.nn.softmax(tf.matmul(x, W) + b)   #预测值，用到softmax\n",
    "\n",
    "\n",
    "#二次代價函數 : loss = mean((y - prediction)^2)\n",
    "#loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction)) #交叉熵\n",
    "\n",
    "#使用梯度下降法\n",
    "#Gradient desent method  (learning rate = 0.2)\n",
    "# gd = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化 代價函數 (operator)\n",
    "# train_step = gd.minimize(loss)\n",
    "#以上两句可以合并为 train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "# train_step = tf.train.AdamOptimizer(0.001)  #0.001学习率\n",
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(loss)     #10^(-2)\n",
    "\n",
    "\n",
    "#初始化变量 operator\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "#测试训练的准确率，求准确率的方法\n",
    "#如果y標籤最大的值，與prediction標籤最大的值相等，則回傳true\n",
    "#結果存在一個 boolean 的变量correct_prediction中\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "                              \n",
    " #argmax 返回一維張量中最大的值所在的位置\n",
    " # 求标签y里面最大的值在哪个位置即标签\n",
    " # tf.argmax(prediction, 1)预测 概率最大就会判定识别的这张图片是属于哪个标签的\n",
    " # (tf.argmax(y, 1)，真实样本的y存放的都是0或1，哪位是1就会返回哪位的值\n",
    " #  然后再比较上面两者，是否一样\n",
    "                             \n",
    "                              \n",
    "# 求准确率\n",
    "# 轉換資料格式 boolean 轉成 32位的float，接著再取平均值，得到准确率\n",
    "# true转换为1.0，false转换为0\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))         \n",
    "                              \n",
    "#開始training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21): #总共疊代21次（周期） (outer loop)，把所有的图片训练21次\n",
    "        #每一次 outer loop 不一次拿所有的數據集，來做 Gradient desent，這就是 stochastic gradient descent\n",
    "        for batch in range(n_batch):#每一個 outer loop 疊代 n_batch 個批次\n",
    "            #利用 train.next_batch 函數，讀取一個batch的 x, y 存給 batch_xs图片数据, batch_ys图片标签\n",
    "            # mnist.train.next_batch(batch_size)是获取下一个一百张图片               \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  \n",
    "                              \n",
    "            feed_dict = {x: batch_xs, y: batch_ys}  #拿來feed 的 dictionary                  \n",
    "            sess.run(train_step, feed_dict)\n",
    "                              \n",
    "        #每做完一次 outer loop 計算一次准确率\n",
    "        outer_loop_feed_dict = {x: mnist.test.images, y: mnist.test.labels} #testing data feed dictionary\n",
    "        acc = sess.run(accuracy, outer_loop_feed_dict)\n",
    "                   \n",
    "        # str(epoch)周期数\n",
    "        print(\"Iter =\" + str(epoch) + \", Testing Accuracy =\" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【注释】\n",
    "#该方法收敛的速度比使用随机下降法速度要快\n",
    "#使用AdamOptimizer的时候，学习率一般都调的比较小\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
